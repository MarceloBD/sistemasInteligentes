General
Bounds
Statistical
Query
Learning
Learning
with
Noise
Hypothesis
Boosting
Javed
Aslam
Lambda
Department
Computer
Science
Dartmouth
College
Hanover
03755
Scott
Decatur
DIMACS
Center
Rutgers
University
Piscataway
08855
Abstract
derive
general
bounds
complexity
learning
Statistical
Query
model
model
with
classification
noise
considering
problem
boosting
accuracy
weak
learning
algorithms
which
fall
within
model
This
model
introduced
Kearns
provide
general
framework
efficient
learning
presence
classification
noise
first
show
general
scheme
boosting
accuracy
weak
learning
algorithms
proving
that
weak
learning
equivalent
strong
learning
boosting
efficient
used
show
main
result
first
general
upper
bounds
complexity
strong
learning
Since
algorithms
simulated
model
with
classification
noise
also
obtain
general
upper
bounds
learning
presence
classification
noise
classes
which
learned
model
References
Angluin
Dana
1992
Computational
learning
theory
Survey
selected
bibliography
Proceedings
Annual
Symposium
Theory
Computing
Angluin
Dana
Philip
Laird
1988
Learning
from
noisy
examples
Machine
Learning
2(4):343--370
Anthony
Martin
Norman
Biggs
1992
Computational
Learning
Theory
Cambridge
Tracts
Theoretical
Computer
Science
Cambridge
University
Press
Aslam
Javed
Scott
Decatur
1994
Improved
noise­tolerant
learning
generalized
statis­tical
queries
Technical
Report
TR­17­94
Harvard
University
July
Aslam
Javed
Scott
Decatur
1995
Specification
simulation
statistical
query
algo­rithms
efficiency
noise
tolerance
Proceedings
Eighth
Annual
Workshop
Computational
Learning
Theory
Press
July
Invited
special
issue
Comp
Blumer
Anselm
Andrzej
Ehrenfeucht
David
Haussler
Manfred
Warmuth
1989
Learn­ability
Vapnik­Chervonenkis
dimension
Journal
36(4):929--865
Drucker
Harris
Robert
Schapire
Patrice
Simard
1992
Improving
performance
neural
networks
using
boosting
algorithm
Advances
Neural
Information
Processing
Systems
Morgan
Kaufmann
Feller
William
1968
Introduction
Probability
Theory
Applications
volume
John
Wiley
Sons
third
edition
Freund
Yoav
1990
Boosting
weak
learning
algorithm
majority
Proceedings
Third
Annual
Workshop
Computational
Learning
Theory
pages
202--216
Morgan
Kaufmann
Freund
Yoav
1992
improved
boosting
algorithm
implications
learning
complexity
Proceedings
Fifth
Annual
Workshop
Computational
Learning
Theory
pages
391--398
Press
Goldman
Sally
Michael
Kearns
Robert
Schapire
1990
sample
complexity
weak
learning
Proceedings
Third
Annual
Workshop
Computational
Learning
Theory
pages
217--231
Morgan
Kaufmann
Graham
Ronald
Donald
Knuth
Oren
Patashnik
1994
Concrete
Mathematics
Founda­tion
Computer
Science
Addison­Wesley
second
edition
Helmbold
David
Robert
Sloan
Manfred
Warmuth
1992
Learning
integer
lattices
SIAM
Journal
Computing
21(2):240--266
Kearns
Michael
1993
Efficient
noise­tolerant
learning
from
statistical
queries
Proceedings
Annual
Symposium
Theory
Computing
pages
392--401
Diego
Laird
Philip
1988
Learning
from
Good
Data
Kluwer
international
series
engi­neering
computer
science
Kluwer
Academic
Publishers
Boston
Sakakibara
Yasubumi
1991
Algorithmic
Learning
Formal
Languages
Decision
Trees
Ph.D
thesis
Tokyo
Institute
Technology
October
International
Institute
Advanced
Study
Social
Information
Science
Fujitsu
Laboratories
Research
Report
IIAS­RR­91­
Sauer
1972
density
families
sets
Journal
Combinatorial
Theory
Series
13:145--147
Schapire
Robert
1990
strength
weak
learnability
Machine
Learning
5(2):197--226
Schapire
Robert
1992
Design
Analysis
Efficient
Learning
Algorithms
Press
Cambridge
Valiant
Leslie
1984
theory
learnable
Communications
27(11):1134--1142
November
Vapnik
A.Ya
Chervonenkis
1971
uniform
convergence
relative
frequencies
events
their
probabilities
Theor
Probability
Appl
16(2):264--280