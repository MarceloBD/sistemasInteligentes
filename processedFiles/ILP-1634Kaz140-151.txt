Combining
LAPIS
WordNet
Learning
Parsers
with
Optimal
Semantic
Constraints
Dimitar
Kazakov
University
York
Heslington
York
YO1O
kazakov@cs.york.ac.uk
home
page
http://www.cs.york.ac.uk/~kazakov
Abstract
There
history
research
focussed
learning
shift-reduce
parsers
from
syntactically
annotated
corpora
means
machine
learning
techniques
based
logic
presence
lexical
semantic
tags
treebank
proved
useful
learning
semantic
constraints
which
limit
amount
nondeterminism
parsers
level
generality
semantic
tags
used
direct
importance
that
task
combine
system
LAPIS
with
lexical
resource
WordNet
learn
parsers
with
semantic
constraints
generality
these
constraints
automatically
selected
LAPIS
from
number
options
provided
corpus
annotator
performance
parsers
learned
evaluated
original
corpus
also
described
article
References
Alfred
Ravi
Sethi
Jeffrey
Uliman
Compilateurs
Principles
techniques
outils
InterEditions
Paris
1989
George
Miller
Introduction
WordNet
on-line
lexical
database
Technical
report
University
Princeton
1993
Dimitar
Kazakov
inductive
approach
natural
language
parser
design
Kemal
Oflazer
Harold
Somers
editors
Proceedings
NeMLaP-2
pages
209—
Ankara
1996
Bilkent
University
Mitchell
Marcus
Beatrice
Santorini
Mary
Marcinkiewicz
Building
large
annotated
corpus
English
Penn
treebank
Computational
Linguistics
1993
Mitchell
Machine
Learning
McGraw-Hill
1997
Plotkin
note
inductive
generalization
Meltzer
Mitchie
editors
Machine
Intelligence
pages
153—163
Edinburgh
University
Press
1970
Samuelsson
Fast
Natural—Language
Parsing
Using
Explanation—Based
Learning
thesis
Royal
Institute
Technology
Stockholm
University
1994
John
Zelle
Using
Inductive
Logic
Programming
Automate
Construction
Natural
Language
Parsers
thesis
University
Texas
Austin
1995
John
Zelle
Raymond
Mooney
Inducing
deterministic
Prolog
parsers
from
treebanks
machine
learning
approach
Proceedings
AAAI-94
pages
748—753
Press/MIT
Press
1994