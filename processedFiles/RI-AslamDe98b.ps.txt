specification
simulation
statistical
query
algorithms
efficiency
noise
tolerance
javed
aslam
department
computer
science
dartmouth
college
hanover
scott
decatur
laboratory
computer
science
massachusetts
institute
technology
cambridge
abstract
recent
innovation
computational
learning
theory
statistical
query
model
advantage
specifying
learning
algorithms
this
model
that
algorithms
simulated
model
both
absence
presence
noise
however
simulations
algorithms
model
have
nonoptimal
time
sample
complexities
this
paper
introduce
method
specifying
statistical
query
algorithms
based
type
relative
error
provide
simulations
noisefree
noisetolerant
models
which
yield
more
efficient
algorithms
requests
estimates
statistics
this
model
take
form
return
estimate
statistic
within
sigma
factor
return
promising
that
statistic
less
than
addition
showing
that
this
very
natural
language
specifying
learning
algorithms
also
show
that
this
specification
polynomially
equivalent
standard
thus
known
learnability
hardness
results
statistical
query
learning
preserved
then
give
highly
efficient
simulations
relative
error
algorithms
show
that
learning
algorithms
obtained
simulating
efficient
relative
error
algorithms
both
absence
noise
presence
malicious
noise
have
roughly
optimal
sample
complexity
also
show
that
simulation
efficient
relative
error
algorithms
presence
classification
noise
yield
learning
algorithms
least
efficient
those
obtained
through
standard
methods
some
cases
improved
roughly
optimal
results
achieved
sample
complexities
these
simulations
based
metric
which
type
relative
error
metric
useful
quantities
which
small
even
zero
show
that
uniform
convergence
with
respect
metric
yields
uniform
convergence
with
respect
accuracy
finally
while
show
that
many
specific
learning
algorithms
written
highly
efficient
relative
error
algorithms
also
show
fact
that
algorithms
written
efficiently
proving
general
upper
bounds
complexity
queries
function
accuracy
parameter
consequence
this
result
give
general
upper
bounds
complexity
learning
algorithms
achieved
through
relative
error
algorithms
simulations
described
above
references
dana
angluin
leslie
valiant
fast
probabilistic
algorithms
hamiltonian
circuits
matchings
journal
computer
system
sciences
april
javed
aslam
scott
decatur
general
bounds
statistical
query
learning
learning
with
noise
hypothesis
boosting
proceedings
annual
symposium
foundations
computer
science
pages
november
appear
information
computation
javed
aslam
scott
decatur
sample
complexity
noisetolerant
learning
information
processing
letters
avrim
blum
merrick
furst
jeffery
jackson
michael
kearns
yishay
mansour
steven
rudich
weakly
learning
characterizing
statistical
query
learning
using
fourier
analysis
proceedings
annual
symposium
theory
computing
anselm
blumer
andrzej
ehrenfeucht
david
haussler
manfred
warmuth
learnability
vapnikchervonenkis
dimension
journal
scott
decatur
statistical
queries
faulty
oracles
proceedings
sixth
nual
workshop
computational
learning
theory
pages
press
july
scott
decatur
learning
hybrid
noise
environments
using
statistical
queries
fisher
lenz
editors
learning
from
data
artificial
intelligence
statistics
springer
verlag
preliminary
version
appeared
proceedings
fifth
international
workshop
artificial
intelligence
statistics
pages
january
scott
decatur
rosario
gennaro
learning
from
noisy
incomplete
examples
proceedings
eigth
annual
workshop
computational
learning
theory
press
july
andrzej
ehrenfeucht
david
haussler
michael
kearns
leslie
valiant
general
lower
bound
number
examples
needed
learning
information
computation
september
yoav
freund
boosting
weak
learning
algorithm
majority
proceedings
third
annual
workshop
computational
learning
theory
pages
morgan
kaufmann
yoav
freund
improved
boosting
algorithm
implications
learning
complexity
proceedings
fifth
annual
workshop
computational
learning
theory
pages
press
david
haussler
quantifying
inductive
bias
learning
algorithms
valiant
learning
framework
artificial
intelligence
david
haussler
decision
theoretic
generalizations
model
neural
other
learning
applications
information
computation
michael
kearns
efficient
noisetolerant
learning
from
statistical
queries
proceedings
annual
symposium
theory
computing
pages
diego
michael
kearns
ming
learning
presence
malicious
errors
siam
journal
computing
philip
laird
learning
from
good
data
kluwer
international
series
engineering
computer
science
kluwer
academic
publishers
boston
pollard
rates
uniform
almostsure
convergence
empirical
processes
indexed
unbounded
classes
functions
manuscript
robert
schapire
strength
weak
learnability
machine
learning
leslie
valiant
theory
learnable
communications
november