Specification
Simulation
Statistical
Query
Algorithms
Efficiency
Noise
Tolerance
Javed
Aslam
Department
Computer
Science
Dartmouth
College
Hanover
03755
Scott
Decatur
Laboratory
Computer
Science
Massachusetts
Institute
Technology
Cambridge
02139
Abstract
recent
innovation
computational
learning
theory
statistical
query
model
advantage
specifying
learning
algorithms
this
model
that
algorithms
simulated
model
both
absence
presence
noise
However
simulations
algorithms
model
have
non­optimal
time
sample
complexities
this
paper
introduce
method
specifying
statistical
query
algorithms
based
type
relative
error
provide
simulations
noise­free
noise­tolerant
models
which
yield
more
efficient
algorithms
Requests
estimates
statistics
this
model
take
form
Return
estimate
statistic
within
Sigma
factor
return
promising
that
statistic
less
than
addition
showing
that
this
very
natural
language
specifying
learning
algorithms
also
show
that
this
specification
polynomially
equivalent
standard
thus
known
learnability
hardness
results
statistical
query
learning
preserved
then
give
highly
efficient
simulations
relative
error
algorithms
show
that
learning
algorithms
obtained
simulating
efficient
relative
error
algorithms
both
absence
noise
presence
malicious
noise
have
roughly
optimal
sample
complexity
also
show
that
simulation
efficient
rela­tive
error
algorithms
presence
classification
noise
yield
learning
algorithms
least
efficient
those
obtained
through
standard
methods
some
cases
im­proved
roughly
optimal
results
achieved
sample
complexities
these
simulations
based
metric
which
type
relative
error
metric
useful
quantities
which
small
even
zero
show
that
uniform
convergence
with
respect
metric
yields
uniform
convergence
with
respect
accuracy
Finally
while
show
that
many
specific
learning
algorithms
written
highly
efficient
relative
error
algorithms
also
show
fact
that
algo­rithms
written
efficiently
proving
general
upper
bounds
complexity
queries
function
accuracy
parameter
consequence
this
result
give
general
upper
bounds
complexity
learning
algorithms
achieved
through
relative
error
algorithms
simulations
described
above
References
Dana
Angluin
Leslie
Valiant
Fast
probabilistic
algorithms
Hamiltonian
circuits
matchings
Journal
Computer
System
Sciences
18(2):155--193
April
1979
Javed
Aslam
Scott
Decatur
General
bounds
statistical
query
learning
learning
with
noise
hypothesis
boosting
Proceedings
Annual
Symposium
Foundations
Computer
Science
pages
282--291
November
1993
appear
Information
Computation
Javed
Aslam
Scott
Decatur
sample
complexity
noise­tolerant
learning
Information
Processing
Letters
57:189--195
1996
Avrim
Blum
Merrick
Furst
Jeffery
Jackson
Michael
Kearns
Yishay
Mansour
Steven
Rudich
Weakly
learning
characterizing
statistical
query
learning
using
Fourier
analysis
Proceedings
Annual
Symposium
Theory
Computing
1994
Anselm
Blumer
Andrzej
Ehrenfeucht
David
Haussler
Manfred
Warmuth
Learnability
Vapnik­Chervonenkis
dimension
Journal
36(4):929
1989
Scott
Decatur
Statistical
queries
faulty
oracles
Proceedings
Sixth
nual
Workshop
Computational
Learning
Theory
pages
262--268
Press
July
1993
Scott
Decatur
Learning
hybrid
noise
environments
using
statistical
queries
Fisher
Lenz
editors
Learning
from
Data
Artificial
Intelligence
Statis­tics
Springer
Verlag
1996
Preliminary
version
appeared
Proceedings
Fifth
International
Workshop
Artificial
Intelligence
Statistics
pages
175--185
January
1995
Scott
Decatur
Rosario
Gennaro
learning
from
noisy
incomplete
examples
Proceedings
Eigth
Annual
Workshop
Computational
Learning
Theory
Press
July
1995
Andrzej
Ehrenfeucht
David
Haussler
Michael
Kearns
Leslie
Valiant
general
lower
bound
number
examples
needed
learning
Information
Compu­tation
82(3):247--251
September
1989
Yoav
Freund
Boosting
weak
learning
algorithm
majority
Proceedings
Third
Annual
Workshop
Computational
Learning
Theory
pages
202--216
Morgan
Kaufmann
1990
Yoav
Freund
improved
boosting
algorithm
implications
learning
com­plexity
Proceedings
Fifth
Annual
Workshop
Computational
Learning
Theory
pages
391--398
Press
1992
David
Haussler
Quantifying
inductive
bias
learning
algorithms
Valiant's
learn­ing
framework
Artificial
Intelligence
36:177--221
1988
David
Haussler
Decision
theoretic
generalizations
model
neural
other
learning
applications
Information
Computation
100:78--150
1992
Michael
Kearns
Efficient
noise­tolerant
learning
from
statistical
queries
Proceedings
Annual
Symposium
Theory
Computing
pages
392--401
Diego
1993
Michael
Kearns
Ming
Learning
presence
malicious
errors
SIAM
Journal
Computing
22(4):807--837
1993
Philip
Laird
Learning
from
Good
Data
Kluwer
international
series
engineering
computer
science
Kluwer
Academic
Publishers
Boston
1988
Pollard
Rates
uniform
almost­sure
convergence
empirical
processes
indexed
unbounded
classes
functions
Manuscript
1986
Robert
Schapire
strength
weak
learnability
Machine
Learning
5(2):197--226
1990
Leslie
Valiant
theory
learnable
Communications
27(11):1134
1142
November
1984