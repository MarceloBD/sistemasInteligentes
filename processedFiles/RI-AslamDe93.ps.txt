general
bounds
statistical
query
learning
learning
with
noise
hypothesis
boosting
javed
aslam
laboratory
computer
science
massachusetts
institute
technology
cambridge
scott
decatur
aiken
computation
laboratory
harvard
university
cambridge
abstract
derive
general
bounds
complexity
learning
statistical
query
model
model
with
classification
noise
considering
problem
boosting
accuracy
weak
learning
algorithms
which
fall
within
statistical
query
model
this
model
introduced
kearns
provide
general
framework
efficient
learning
presence
classification
noise
first
show
general
scheme
boosting
accuracy
weak
learning
algorithms
proving
that
weak
learning
equivalent
strong
learn
boosting
eficient
used
show
main
result
first
general
upper
bounds
complexity
strong
learning
specifically
derive
simultaneous
upper
bounds
with
spect
number
queries
vapnikchervonenkis
dimension
query
space
inverse
minimum
erance
addition
show
that
these
general
upper
bounds
nearly
optimal
describing
class
learning
problems
which
simultaneously
lower
bound
number
queries
inverse
minimum
tolerance
further
apply
boosting
results
model
learning
model
with
classification
noise
since
nearly
learning
algorithms
cast
model
apply
boosting
techniques
convert
these
algorithms
into
highly
eficient
algorithms
simulating
these
eficient
algorithms
model
with
classification
noise
show
that
nearly
algorithms
converted
into
highly
eficient
algorithms
which
tolerate
classification
noise
give
upper
bound
sample
complexity
these
noisetolerant
algorithms
which
nearly
optimal
with
respect
noise
rate
also
give
upper
bounds
space
complexity
hypothesis
size
show
that
these
measures
fact
independent
noise
rate
note
that
running
times
these
noisetolerant
algorithms
eficient
this
sequence
simulations
also
demonstrates
that
possible
boost
accuracy
nearly
algorithms
even
presence
noise
this
provides
partial
answer
open
problem
schapire
first
theoretical
evidence
empirical
result
drucker
schapire
simard
references
dana
angluin
computational
learning
theory
survey
selected
bibliography
proceedings
twentyfourth
annual
symposium
theory
computing
pages
dana
angluin
philip
laird
learning
from
noisy
examples
machine
learning
scott
decatur
statistical
queries
faulty
oracles
proceedings
sixth
annual
workshop
computational
learning
theory
press
harris
drucker
robert
schapire
patrice
simard
improving
performance
neural
networks
using
boosting
algorithm
advances
neural
information
processing
systems
morgan
kaufmann
andrzej
ehrenfeucht
david
haussler
michael
kearns
leslie
valiant
general
lower
bound
number
examples
needed
learning
information
computation
september
yoav
freund
boosting
weak
learning
algorithm
majority
proceedings
third
annual
workshop
computational
learning
theory
pages
morgan
kaufmann
yoav
freund
improved
boosting
algorithm
implications
learning
complexity
proceedings
fifth
annual
workshop
computational
learning
theory
pages
press
yoav
freund
personal
communication
sally
goldman
michael
kearns
robert
schapire
exact
identification
circuits
using
fixed
points
amplification
functions
proceedings
symposium
foundations
computer
science
pages
ieee
october
sally
goldman
michael
kearns
robert
schapire
sample
complexity
weak
learning
proceedings
colt
pages
morgan
kaufmann
david
helmbold
robert
sloan
manfred
warmuth
learning
integer
lattices
siam
journal
computing
michael
kearns
cient
noisetolerant
learning
from
statistical
queries
proceedings
twentyfifth
annual
symposium
theory
computing
philip
laird
learning
from
good
data
kluwer
international
series
engineering
puter
science
kluwer
academic
publishers
boston
yasubumi
sakakibara
algorithmic
learning
languages
decision
trees
thesis
tokyo
institute
technology
october
international
institute
advanced
study
social
information
science
fujitsu
laboratories
research
report
iiasrr
robert
schapire
strength
weak
learnability
machine
learning
robert
schapire
design
analysis
efficient
learning
algorithms
press
cambridge
hans
ulrich
simon
general
bounds
number
examples
needed
learning
probabilistic
concepts
proceedings
sixth
annual
workshop
computational
learning
theory
press
leslie
valiant
theory
learnable
communications
november