Noise
Detection
Elimination
Applied
Noise
Handling
Chess
Endgame
Dragan
Gamberger1
Nada
Lavrac2
Rudjer
Boskovic
Institute
Bijenicka
10000
Zagreb
Croatia
4561142
425497
gambi@lelhp1.irb.hr
Jozef
Stefan
Institute
Jamova
1000
Ljubljana
Slovenia
1773272
219385
nada.lavrac@ijs.si
Abstract
Compression
measures
used
inductive
learners
such
measures
based
Minimum
Description
Length
principle
provide
theoretically
justified
basis
grading
candidate
hypotheses
Compression-based
induction
appropriate
also
handling
noisy
data
This
paper
shows
that
simple
compression
measure
used
detect
noisy
examples
technique
proposed
which
noisy
examples
detected
eliminated
from
training
hypothesis
then
built
from
remaining
examples
separation
noise
detection
hypothesis
formation
advantage
that
noisy
examples
influence
hypothesis
construction
opposed
most
standard
approaches
noise
handling
which
learner
typically
tries
avoid
overfitting
noisy
example
Experimental
results
king-rook-king
chess
endgame
domain
show
potential
this
novel
approach
noise
handling
References
Cestnik
Bratko
estimating
probabilities
tree
pruning
Proc
European
Working
Session
Learning
138—150
1991
Clark
Niblett
induction
algorithm
Machine
Learning
3(4):261—283
1989
Jong
Spears
Learning
concept
classification
rules
using
genetic
algorithms
Proceedings
12th
International
Joint
Conference
Artificial
Intelligence
IJCAI-91
Morgan
Kaufmann
1991
651—656
Dzeroski
Lavrac
Inductive
learning
deductive
databases
IEEE
Transactions
Knowledge
Data
Engineering
939—949
1993
Gamberger
minimization
approach
propositional
inductive
learning
Proceeding
European
Conference
Machine
Learning
ECML-95
Springer
1995
151—160
Gamberger
Specific
rule
induction
medical
domains
Proc
Computer-Aided
Data
Analysis
Medicine
CADAM-95
136—145
Scientific
Publishing
IJS-SP-95-1
1995
Gamberger
Lavrac
Towards
theory
relevance
inductive
concept
learning
Technical
report
IJS-DP-7310
Stefan
Institute
Ljubljana
1995
Gamberger
Lavrac
Dzeroski
Noise
elimination
inductive
concept
learning
case
study
medical
diagnosis
Proc
Seventh
International
Workshop
Algorithmic
Learning
Theory
ALT’96
Springer
1996
press
Lavrac
Dzeroski
Grobelnik
Learning
nonrecursive
definitions
relations
with
LINUS
Proc
Fifth
European
Working
Session
Learning
pages
265—281
Springer
Berlin
1991
Lavrac
Dzeroski
Inductive
learning
relations
from
noisy
examples
Muggleton
Inductive
Logic
Programming
495-516
Academic
Press
1992
Lavrac
Dzeroski
Inductive
Logic
Programming
Technique
Applications
Ellis
Horwood
Simon
Schuster
Ellis
Horwood
Series
Artificial
Intelligence
Chichester
1994
Lavrac
Gamberger
Dzeroski
approach
dimensionality
reduction
learning
from
deductive
databases
Proceedings
International
Workshop
Inductive
Logic
Programming
ILP-95
Technical
report
Katholieke
Universiteit
Leuven
1995
Lavrac
Dzeroski
Bratko
Handling
imperfect
data
inductive
logic
programming
Raedt
Advances
Inductive
Logic
Programming
48-64
Press
1996
Mingers
empirical
comparison
pruning
methods
decision
tree
induction
Machine
Learning
4(2):227—243
1989
Mingers
empirical
comparison
selection
measures
decision-tree
induction
Machine
Learning
3(4):319—342
1989
Muggleton
Bain
Hayes-Michie
D.Michie
experimental
comparison
human
machine
learning
formalisms
Proc
Sixth
International
Workshop
Machine
Learning
113—118
Morgan
Kaufmann
Mateo
1989
Muggleton
Srinivasan
Bain
Compression
significance
accuracy
Proc
International
Conference
Machine
Learning
338—347
Morgan
Kaufmann
1992
Niblett
Bratko
Learning
decision
rules
noisy
domains
Bramer
Research
Development
Expert
Systems
24—25
Cambridge
University
Press
1986
Quinlan
Simplifying
decision
trees
International
Journal
Man-Machine
Studies
27(3):221—234
1987
Quinlan
Learning
logical
definitions
from
relations
Machine
Learning
239—266
1990
Rissanen
Modeling
shortest
data
description
Automatica
465—471
1978